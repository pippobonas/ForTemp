networks:
  for_temp:
    name: for_temp
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
    
services:
  setup_template:
    image: curlimages/curl:latest
    profiles: ["setup"]
    networks:
      for_temp:
        ipv4_address: 172.20.0.6
    depends_on:
      - lek_ela
    entrypoint: >
      sh -c "
        echo 'Waiting for Elasticsearch...' &&
        until curl -s http://172.20.0.5:9200 >/dev/null; do
          sleep 5;
        done;
        echo 'Creating the template...' &&
        curl -X PUT http://172.20.0.5:9200/_index_template/weather-current -H 'Content-Type: application/json' -d @/usr/share/elasticsearch/config/mapping_template_weather_current.json &&
        curl -X PUT http://172.20.0.5:9200/_index_template/weather-forecast -H 'Content-Type: application/json' -d @/usr/share/elasticsearch/config/mapping_template_weather_forecast.json "  
    volumes:
      - ./Elastic_template/config/mapping_template_weather_current.json:/usr/share/elasticsearch/config/mapping_template_weather_current.json
      - ./Elastic_template/config/mapping_template_weather_forecast.json:/usr/share/elasticsearch/config/mapping_template_weather_forecast.json
  
  trainer:
    build:
      context: spark/
      dockerfile: Dockerfile
    container_name: trainer
    profiles: ["train"]
    volumes:
      - ./data_acquisation/data_acquisation_batch/output.csv:/app/dati/output.csv
      - ./spark/pipeline_model2:/app/pipeline_model
      - ./spark/utils.py:/app/utils.py
      - ./spark/spark-batch.py:/app/consumer.py
    command: /opt/spark/bin/spark-submit /app/consumer.py

  acquisition:
    build:
      context: data_acquisition/data_acquisition_stream
      dockerfile: Dockerfile  
    
    depends_on:
      - lek_log
    env_file:
      - data_acquisition/data_acquisition_stream/acquisition.env
    environment:
      - SEND_SERVER= http://lek_log:5044
    volumes:
      - ./data_acquisition/data_acquisition_stream/data:/app/data
    networks:
          for_temp:
            ipv4_address: 172.20.0.12
    restart: always

  lek_log:
    image: docker.elastic.co/logstash/logstash:8.16.0
    container_name: lek_log
    depends_on:
      - bk1
    volumes:
      - ./logstash/pipeline/:/usr/share/logstash/pipeline/
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
      - ./logstash/config/pipelines.yml:/usr/share/logstash/config/pipelines.yml
    environment:
      - "XPACK_MONITORING_ENABLED=false"
    ports:
      - "5044:5044"
    networks:
      for_temp:
        ipv4_address: 172.20.0.11
    restart: always

  bk1:
    image: apache/kafka:3.9.0
    container_name: bk1
    user: "1000:1000"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://bk1:9092,CONTROLLER://bk1:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://bk1:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@bk1:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_LOG_RETENTION_MS: 86400000
    command: >
      sh -c "/etc/kafka/docker/run & 
      sleep 10 &&
      /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --topic current-weather --bootstrap-server bk1:9092 && 
      /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --topic weather-data-aggregated --bootstrap-server bk1:9092 && 
      /opt/kafka/bin/kafka-topics.sh --create --if-not-exists --topic forecast-2t-1h --bootstrap-server bk1:9092 &&
      wait" 
    ports:
      - "9092:9092"
      - "9093:9093"
    networks:
      for_temp:
        ipv4_address: 172.20.0.2
    restart: always

  kafka_stream:
    build:
      context: kafka_stream_code/
      dockerfile: Dockerfile
    container_name: kafka_stream
    user: "1000:1000"
    environment:
      KAFKA_BOOTSTRAP_SERVERS: bk1:9092
      WINDOW_SIZE_HOURS: 4
      WINDOW_SIZE_PLUS_MINUTES: 40
      WINDOW_ADVANCE_HOUES: 1
    depends_on:
      - bk1
    networks:
      for_temp:
        ipv4_address: 172.20.0.3
    restart: always

  spark_stream:
    build:
      context: spark/
      dockerfile: Dockerfile
    container_name: spark_stream
    networks:
      for_temp:
        ipv4_address: 172.20.0.4
    depends_on:
      - kafka_stream
    volumes:
      - ./spark/pipeline_model2:/app/pipeline_model
      - ./spark/utils.py:/app/utils.py
      - ./spark/spark-streaming.py:/app/consumer.py
      - ./spark/ivy2:/home/spark/.ivy2
    command: /opt/spark/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.4,org.apache.kafka:kafka-clients:3.9.0 /app/consumer.py
    ports:
      - "4040:4040"
    restart: always


  lek_ela:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.0
    container_name: lek_ela
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g       -Xmx1g"
    ports:
      - "9200:9200"
    networks:
      for_temp:
        ipv4_address: 172.20.0.5
    volumes:
      - ES-data:/usr/share/elasticsearch/data
    restart: always

  lek_kib:
    image: docker.elastic.co/kibana/kibana:8.17.0
    container_name: lek_kib
    networks:
      for_temp:
        ipv4_address: 172.20.0.7
    environment:
      - ELASTICSEARCH_HOSTS=http://lek_ela:9200
      - xpack.security.enabled=false
    depends_on:
      - lek_ela
    ports:
      - "5601:5601"
    volumes:
      - kibana-data:/usr/share/kibana/data
    restart: always

volumes:
  kibana-data:
  ES-data:
  

  